%Problem Set 1 LaTeX report for TDT4200
\documentclass[fontsize=11pt, paper=a4, titlepage]{article}
\input{../../config} %config.tex file in same directory for all reports

\begin{document}

\begin{center}

{\huge Problem Set 4, Theory}\\[0.5cm]

\textsc{\LARGE TDT4200 -}\\[0.5cm]
\textsc{\large Parallel Computations}\\[1.0cm]

\begin{table}[h]
    \centering
    \begin{tabular}{c}
        \textsc{Christian Chavez}
    \end{tabular}
\end{table}

\end{center}
\vfill
\hfill \large{\today}
\clearpage

\section{Problem 1, Multithreading}
\begin{enumerate}[a)]

    \item \textit{MPI} is a library of functions used to achieve parallelism
primarily through processes, not threads, and thus is best suited a distributed
memory environment.

\textit{OpenMP} is both a library and a C extension which gives the programmer
library functions to achieve thread-level parallelism, as well as
\lstinline!#pragmas! which can tell the compiler to parallelize certain portions
of the code.

\textit{Pthreads} (\textbf{P}osix \textbf{threads}) is a standardized
programming interface which hardware vendors implemented to allow programmers to
program with threads.

    \item A shared memory system is a memory system where all cores/processors
share cache/main memory. (Or equivalently across different aspects of the memory
hierarchy).

A distributed memory system is when (in the aforementioned examples), the
cache/main memory is spread across all the different cores/processors, but still
interconnected.

    \item Distributed memory has the drawback of achieving synchronicity of all
the distributed copies of data, and hence a very complex memory management
system for it to look like a ``shared memory system'' for the software
developers using the system. However, it has the benefit of being simpler to
implement hardware-wise, and the complex parts of the memory management system
does not have to be implemented in hardware, it can be implemented in software.
Hence, it's also easier to add/remove memory from this system without having to
take the whole system down. The communication system in a distributed memory
system is a lot bigger than the memory system for a shared memory however, and
this is often seen as a resource-costly drawback.

A shared memory system has the drawback of managing the atomicity of the
data/variables in memory, something a distributed system does not have to worry
about, since in a distributed system all the cores/processors have each their
own memory/cache with a local copy of the variable to work on. As mentioned
however, the distributed system does need to enable atomicity through
communication (called synchronicity above), which a shared system does not need
to worry about, since there only exists one copy.

In an attempt to sum up: Shared memory systems need only $M$ lines of
communication, where $M$ is the amount of processors/cores, while distributed
memory needs upto $M^2$ lines of communication (here's where the synchronicity
complexity comes into play when we try to minimize this cost by implementing
topologies like hypercubes).

Shared memory systems need to make sure that two ``actors'' don't access the
same data at the same time, while a distributed memory system needs to update
all the distributed local copies of data throughout its network whenever an
``actor'' changes its local copy.

\end{enumerate}

\section{Problem 2, Load balancing}
\begin{enumerate}[a)]

    \item OpenMP has three different for-loop scheduling clauses:
    \begin{enumerate}[1)]

        \item \textbf{static} allocates iterations to all the threads before
they execute their loop-iterations. By default, iterations are divided equally,
but \textit{chunk} can specify a number of contiguous iterations for each
thread.

        \item \textbf{dynamic} allocates some iterations to a smaller number of
threads. Once any thread finishes its allocated iterations, it checks and
retrieves another iteration if there are more left. \textit{chunk} specifies the
number of contiguous iterations given to any thread at start or when a thread
requests more work.

        \item \textbf{guided} A large portion of contiguous threads are
allocated to  each thread like in \textbf{dynamic} (above). The portion decreases exponentially with each successive allocation of iterations down to a minimum size specified in the parameter \textit{chunk}.

    \end{enumerate}

    \item Assumption \textbf{1}: There's no communication cost regarding/due to
scheduling, like when a thread finishes its allocated iterations and
checks/requests new ones.

Assumption \textbf{2}: There's no interdependency between iterations whatsoever.
Including accessing functions \lstinline!a()!, \lstinline!b()!, \lstinline!c()!,
aswell as \lstinline!table[]!.

Assumption \textbf{3}: The time to execute two different iterations with the
same function calls is identical no matter what the iteration count is.

    \begin{enumerate}[I)]

        \item\begin{align*}
            \frac{1024\times a(i)}{Amount\thickspace of\thickspace threads} &=
            \frac{1024\times t}{8}\\
            &= 128t
        \end{align*}
        Since \textit{chunk size} equals $128$, the time it will take to finish
with $8$ threads is the time it takes for one thread to execute $128t$.

        \item\begin{align*}
            \frac{1024\times b(i)}{Amount\thickspace of\thickspace threads} &=
            \frac{1024\times it}{8}\\
            &= 128it
        \end{align*}
        Since \textit{chunk size} equals $128$, the time it will take to finish
with $8$ threads is the time it takes for one thread to execute $128it$.

        \item\begin{align*}
            1024\times b(i) &= 1024it
        \end{align*}

        Since we're using a dynamic schedule  with \textit{chunk size} equaling
$100$, it means that all the $8$ threads are allocated $100$ iterations at a
time, hence:

        \begin{align*}
            \frac{1024it}{100} &= 10.24it = T_{WorkPerChunk}\\
            \frac{T_{TotalWork}}{T_{WorkPerChunk}} &= \frac{1024it}{10.24it} = 100 Chunks
        \end{align*}

        Execution time for this example equals the the amount of \textit{chunks}
divided by the number of available threads; $\frac{100}{8}= 12.5$. This means
that it's the work of a chunk multiplied with the amount of chunks per thread
which decided total execution time: $13\times 10.24it = 162.5$. \\

Not all threads will need all this time, but half of them will, since the $.5$
in $12.5$ means that half the threads will run a full $T_{WorkPerChunk}$ extra
to finish.

        \item\begin{align*}
            T_{TotalWork} &= 1024\times i^2t = 1024i^2t
        \end{align*}

        Since this example uses \textbf{guided}, that means it will use the
following chunk sizes (in order) to split the iteration's workload: $\{512, 256,
128, 64, 32, 16, 8, 4, 2, 1, 1\}$.

Hence, the first thread will be given $512i^2t$ work to be done, the next
available thread will be given $256i^2t$, and so on. The limiting factor here
will be that there are only $8$ threads available. With no overhead due to
scheduling, and the above list being $11$ elements long, this implies that the
slowest thread to finish will be the first one. The one receiving $512i^2t$. \\


Thus, this example will take $512i^2t$ to execute.

    \end{enumerate}

    \item \begin{align*}
            T_{TotalWork} &= 1000\times(t+it) = 1000it+1000t \\
          T_{WorkPerThread} &= \frac{1000(t+it)}{8} = 125(t+it)
        \end{align*}

        Since $1000\%8 == 0$, given assumption \textbf{3}, the best
loadbalancing would be to assign an equal amount of contiguous
iterations to each thread, namely $125$ each.

\end{enumerate}

\section{Problem 3, Deadlocks and Races}
\begin{enumerate}[a)]

    \item A \textit{race condition} occurs when two ``actors'' (e.g. threads)
access a  shared variable at the same time, such that one actor has updated the
value of the shared variable before the second actor uses the value, but after
it has read the value (accessed the variable). Or similarly (and hence the
name), they both operate on the variable, returning different ``new'' values for
it, racing to see who gets to write the value to the variable last. This since
the last write will be the one that remains after the two ``actors'' access to
the shared variable.

    \item A \textit{deadlock} is when two ``actors'' each lock one of two
variables  such that no other ``actors''can use it, and then both wait for the
other to finish with their respective locked variables so that both can
continue. As a result, each ``actor'' waits indefinitely for the other to
finish, completely stopping (freezing) the program flow, making it hang.

    \item Deadlock or race condition present:
    \begin{enumerate}[I)]

        \item There's a race condition present on \lstinline!j!, since the line
containing (nor the variable itself) \lstinline!j++;! is not protected from such accesses. There is no deadlock present.

        \item Neither a deadlock or race condition is present. However, this
code will  never complete, resulting in the program hanging. There is nothing
modifying the variable \lstinline§i§ such that it at some points fails the
while-loop check.

        \item \todo[inline]{NO CLUE!!!}

        \item \todo[inline]{NO CLUE!!!}

    \end{enumerate}
\end{enumerate}

\section{Problem 4, Reduction}
\begin{enumerate}[a)]
    \item \begin{enumerate}[A]
        \item \begin{align*}
            (2\times 15) + 1 &= 30 + 1 = 31
        \end{align*}

        \item \begin{align*}
            (4\times 7) + 3 &= 28 + 3 = 31
        \end{align*}

        \item \begin{align*}
            (8\times 3) + (2\times 3) + 1 &= 24 + 6 + 1 = 31
        \end{align*}

        \item \begin{align*}
            (16\times 1) + (8\times 1) + (4\times 1) + (2\times 1) + 1 &=
            16 + 8 + 4 + 2 + 1 = 31
        \end{align*}
    \end{enumerate}

    \item Assumption: The maximum number of threads operating in parallel is
based on the maximum of amount of discrete blocks on an horizontal level for
each subproblem.
    \begin{enumerate}[A]
    \item $2$

    \item $4$

    \item $8$

    \item $16$

    \end{enumerate}

    \item Assumption \textbf{1}: The definition of ``best'' is the fastest
alternative. \\
    Assumption \textbf{2}: The time it takes to perform an addition is
constant, $C$. \\
    Assumption \textbf{3}: There is no overhead in reads/writes for the threads,
and they share memory space so no transfer is needed. \\
    Assumption \textbf{4}: There is a small cost of assigning work to threads,
but this  is constant, independent of size of workload assigned to thread. This
constant is denoted as $t$.
    \begin{enumerate}[A]
        \item $2(15C+t) + (C+t) = 30C+2t+C+t = 31C + 3t$
        \item $4(7C+t) + (3C+t) = 28C+4t+3C+t = 31C + 5t$
        \item $8(3C+t) + 2(3C+t) + (C+t) = 24C+8t+6C+2t+C+t = 31C + 11t $
        \item $16(C+t) + 8(C+t) + 4(C+t) + 2(C+t) + (C+t) = (16+8+4+2+1)(C+t) =
        31(C+t)$
    \end{enumerate}

\end{enumerate}

\vfill
\hfill \large{\today}
\end{document}
