%Problem Set 1 LaTeX report for TDT4200
\documentclass[fontsize=11pt, paper=a4, titlepage]{article}
\input{../../config} %config.tex file in same directory for all reports

\begin{document}

\begin{center}

{\huge Problem Set 4, Theory}\\[0.5cm]

\textsc{\LARGE TDT4200 -}\\[0.5cm]
\textsc{\large Parallel Computations}\\[1.0cm]

\begin{table}[h]
    \centering
    \begin{tabular}{c}
        \textsc{Christian Chavez}
    \end{tabular}
\end{table}

\end{center}
\vfill
\hfill \large{\today}
\clearpage

\section{Problem 1, Multithreading}
\begin{enumerate}[a)]

    \item \textit{MPI} is a library of functions used to achieve parallelism
primarily through processes, not threads, and thus is best suited a distributed
memory environment.

\textit{OpenMP} is both a library and a C extension which gives the programmer
library functions to achieve thread-level parallelism, as well as
\lstinline!#pragmas! which can tell the compiler to parallelize certain portions
of the code.

\textit{Pthreads} (\textbf{P}osix \textbf{threads}) is a standardized
programming interface which hardware vendors implemented to allow programmers to
program with threads.

    \item A shared memory system is a memory system where all cores/processors
share cache/main memory. (Or equivalently across different aspects of the memory
hierarchy).

A distributed memory system is when (in the aforementioned examples), the
cache/main memory is spread across all the different cores/processors, but still
interconnected.

    \item Distributed memory has the drawback of achieving synchronicity of all
the distributed copies of data, and hence a very complex memory management
system for it to look like a ``shared memory system'' for the software
developers using the system. However, it has the benefit of being simpler to
implement hardware-wise, and the complex parts of the memory management system
does not have to be implemented in hardware, it can be implemented in software.
Hence, it's also easier to add/remove memory from this system without having to
take the whole system down. The communication system in a distributed memory
system is a lot bigger than the memory system for a shared memory however, and
this is often seen as a resource-costly drawback.

A shared memory system has the drawback of managing the atomicity of the
data/variables in memory, something a distributed system does not have to worry
about, since in a distributed system all the cores/processors have each their
own memory/cache with a local copy of the variable to work on. As mentioned
however, the distributed system does need to enable atomicity through
communication (called synchronicity above), which a shared system does not need
to worry about, since there only exists one copy.

In an attempt to sum up: Shared memory systems need only $M$ lines of
communication, where $M$ is the amount of processors/cores, while distributed
memory needs upto $M^2$ lines of communication (here's where the synchronicity
complexity comes into play when we try to minimize this cost by implementing
topologies like hypercubes).

Shared memory systems need to make sure that two ``actors'' don't access the
same data at the same time, while a distributed memory system needs to update
all the distributed local copies of data throughout its network whenever an
``actor'' changes its local copy.

\end{enumerate}

\section{Problem 2, Load balancing}
\begin{enumerate}[a)]

    \item OpenMP has three different for-loop scheduling clauses:
    \begin{enumerate}[1)]

        \item \textbf{static} allocates iterations to all the threads before
they execute their loop-iterations. By default, iterations are divided equally,
but \textit{chunk} can specify a number of contiguous iterations for each
thread.

        \item \textbf{dynamic} allocates some iterations to a smaller number of
threads. Once any thread finishes its allocated iterations, it checks and
retrieves another iteration if there are more left. \textit{chunk} specifies the
number of contiguous iterations given to any thread at start or when a thread
requests more work.

        \item \textbf{guided} A large portion of contiguous threads are
allocated to  each thread like in \textbf{dynamic} (above). The portion decreases exponentially with each successive allocation of iterations down to a minimum size specified in the parameter \textit{chunk}.

    \end{enumerate}

    \item Assumption \textbf{2}: There's no interdependency between iterations
whatsoever. Including accessing functions \lstinline!a()!, \lstinline!b()!,
\lstinline!c()!, aswell as \lstinline!table[]!.

    \begin{enumerate}[I)]

        \item\begin{align*}
            \frac{1024\times a(i)}{Amount\thickspace of\thickspace threads} &=
            \frac{1024\times t}{8}\\
            &= 128t
        \end{align*}
Since \textit{chunk size} equals $128$, the time it will take to finish with $8$
threads is the time it takes for one thread to execute $128t$.

        \item\begin{align*}
            T_{TotalWork} &= \sum_{i=0}^{1023} b(i) = \sum_{i=0}^{1023} it
        \end{align*}
This one is almost identical to the previous one, except that the work per
thread is not constant. Hence, the since, we're dealing with the same number of
iterations, but different workload, the time it takes to complete will be the
time it takes to complete the largest workload is $\sum_{i=896}^1023 it$.

        \item\begin{align*}
            T_{TotalWork} &= \sum_{i=0}^{1023} b(i) = \sum_{i=0}^{1023} it
        \end{align*}

Since we're using a dynamic schedule with \textit{chunk size} equaling $100$, it
means that all the $8$ threads are allocated $100$ iterations at a time. Hence,
since there's only $10.24$ multiples of $100$ in $1024$, and there's $8$ threads
available, it means that there will be $3$ threads that will work through two
\textit{chunks}/workloads.

Again, like in the previous, the last chunk of $100$ iterations will be the
slowest one (because the absolute last one is only $24$ elements large), which
implies that the last thread to get assigned a second \textit{chunk} will be the
one to run this workload which will dominate the run-time required. Since
there's $3$ ``extra'' \textit{chunks}/workloads, the thread that gets the
workload for $\sum_{i=100}^199 it$, will be the one to receive the largest
workload of them all, namely $\sum_{i=900}^999 it$.

As such, the time it will take to execute will be:
$\sum_{i=100}^{199} it + \sum_{i=900}^{999} it = 109\thickspace 900t$

        \item\begin{align*}
            T_{TotalWork} &= \sum_{i=0}^{1023} i^2t =
            357\thickspace 389\thickspace 824t
        \end{align*}

Since this example uses \textbf{guided}, that means it will use the following
chunk sizes of iterations to split the iteration's workload (in order): $\{512,
256, 128, 64, 32, 16, 8, 4, 2, 1, 1\}$.

Hence, the first thread will be given $\sum_{i=0}^{511}i^2t$ work to be done,
the next available thread will be given $\sum_{i=512}^{767}i^2t$, and so on.

There are two factors to consider in this problem; (1) the amount of threads
($8$) is less than the amount of chunks that will be handed out to the threads
($11$), and (2) the chunks are of varying time requirements (depending on the
value of $i$).

So since the largest work done by any one thread will either be two chunks, or
the largest chunk of them all (if such a chunk exists). Hence, of the three
``extra'' chunks which will be executed by a thread that's already done a chunk,
are the ones numbered ${2, 1, 1}$ above. Since $\sum_{i=100}^{1021}i^2t$ is
bigger than both $1022^2t$ and $1023^2t$, it's only the $9th$ chunk we need to
compare with the biggest chunk of them all.

The biggest chunk of them all will likely be in among the first three, as these
last three show. The growth rate between each chunk does not change between the
chunks, so there's no reason to check them all if we find one that is bigger
than its neighbours. \\

$\sum_{i=0}^{511}i^2t (\approx 44.6 \textrm{million}) < \sum_{i=512}^{767}i^2t
(\approx 106 \textrm{million})$, and $106 \textrm{million} > \sum_{i=768}^{895}i
^2t (\approx 88.7 \textrm{million}$. \\

So, since $\sum_{i=0}^{511} i^2t + \sum_{i=1020}^{1021} i^2t <
\sum_{i=512}^{767} i^2t$, the time it will take a thread to execute the $2nd$
chunk, of size $256$, will be the most time consuming workload.

    \end{enumerate}

    \item \begin{align*}
        T_{TotalWork} &= \sum_{i=0}^{999} t+it = t\sum_{i=0}^{999} (i+1)
    \end{align*}

Since $\frac{1000}{8} = 125$ we could have just divided the iterations equally
if each iteration had cost the same amount of time as any other. This not being
the case, the best way to assign the iterations is to divide up the iterations
into $2\times 8 = 16$ chunks, and have thread $j$ be assigned half of the first
$125$ iterations, as well the last half of the last $125$. The next thread $j+1$
should then be assigned the second half of the first $125$ iterations, and the
first half of the last $125$ iterations. And so on, until the $8th$ thread takes
the iterations from number $375$/$374$ to $624$/$625$, depending on how you index the iterations.

\end{enumerate}

\section{Problem 3, Deadlocks and Races}
\begin{enumerate}[a)]

    \item A \textit{race condition} occurs when two ``actors'' (e.g. threads)
access a  shared variable at the same time, such that one actor has updated the
value of the shared variable before the second actor uses the value, but after
it has read the value (accessed the variable). Or similarly (and hence the
name), they both operate on the variable, returning different ``new'' values for
it, racing to see who gets to write the value to the variable last. This since
the last write will be the one that remains after the two ``actors'' access to
the shared variable.

    \item A \textit{deadlock} is when two ``actors'' each lock one of two
variables  such that no other ``actors''can use it, and then both wait for the
other to finish with their respective locked variables so that both can
continue. As a result, each ``actor'' waits indefinitely for the other to
finish, completely stopping (freezing) the program flow, making it hang.

    \item Deadlock or race condition present:
    \begin{enumerate}[I)]

        \item There's a race condition present on \lstinline!j!, since the line
containing (nor the variable itself) \lstinline!j++;! is not protected from such
accesses. There is no deadlock present.

        \item Neither a deadlock or race condition is present. However, this
code will never complete, resulting in the program hanging. There is nothing
modifying the variable \lstinline§i§ such that it at some points fails the
while-loop check.

        \item No deadlock or race condition present.

        \item No deadlock or race condition present.

    \end{enumerate}
\end{enumerate}

\section{Problem 4, Reduction}
\begin{enumerate}[a)]
    \item \begin{enumerate}[A]
        \item \begin{align*}
            (2\times 15) + 1 &= 30 + 1 = 31
        \end{align*}

        \item \begin{align*}
            (4\times 7) + 3 &= 28 + 3 = 31
        \end{align*}

        \item \begin{align*}
            (8\times 3) + (2\times 3) + 1 &= 24 + 6 + 1 = 31
        \end{align*}

        \item \begin{align*}
            (16\times 1) + (8\times 1) + (4\times 1) + (2\times 1) + 1 &=
            16 + 8 + 4 + 2 + 1 = 31
        \end{align*}
    \end{enumerate}

    \item Assumption: The maximum number of threads operating in parallel is
based on the maximum of amount of discrete blocks on an horizontal level for
each subproblem.
    \begin{enumerate}[A]
    \item $2$

    \item $4$

    \item $8$

    \item $16$

    \end{enumerate}

    \item Assumption \textbf{1}: The definition of ``best'' is the fastest
alternative. \\
    Assumption \textbf{2}: The time it takes to perform an addition is
constant, $C$. \\
    Assumption \textbf{3}: There is no overhead in reads/writes for the threads,
and they share memory space so no transfer is needed. \\
    Assumption \textbf{4}: Amdahl's law is valid here, since the workload is the
same for all alternatives A, B, C, and D.

    It's given that for each horizontal level, if the discrete blocks can be added
up in parallel, it will be faster to do so than having all of the additions per
horizontal level done in sequence. Hence, only the max nr. of threads per
alternative will be considered for said alternative:

    \begin{enumerate}[A]
    \item $2(15C) + C \Rightarrow 15C + C = 16C$
    \item $4(7C) + 3C \Rightarrow 7C + 3C = 10C$
    \item $8(3C) + 2(3C) + C \Rightarrow 3C + 3C + C = 7C $
    \item $16C + 8C + 4C + 2C + C \Rightarrow C + C + C + C = 4C$
    \end{enumerate}

    Alternative D with $16$ threads is then the fastest alternative in terms of
multiples of $C$.

\end{enumerate}

\vfill
\hfill \large{\today}
\end{document}
