%Problem Set 1 LaTeX report for TDT4200
\documentclass[fontsize=11pt, paper=a4, titlepage]{article}
\input{../../config} %config.tex file in same directory for all reports

\begin{document}

\begin{center}

{\huge Problem Set 6, Theory}\\[0.5cm]

\textsc{\LARGE TDT4200 -}\\[0.5cm]
\textsc{\large Parallel Computations}\\[1.0cm]

\begin{table}[h]
    \centering
    \begin{tabular}{c}
        \textsc{Christian Chavez}
    \end{tabular}
\end{table}

\end{center}
\vfill
\hfill \large{\today}
\clearpage

\section*{Problem 1, OpenCL}
    \begin{enumerate}[a)]

        \item ``\textit{OpenCL} is to vehicles, like \textit{CUDA} is to cars'',
to paraphrase the famous ``Java is to javascript like cars are to carpets''.
OpenCL is meant for and used to program across many different platforms, where a
subset of said platforms include (among other things) Accelerators, FPGAs, GPUs,
and Nvidia GPUs. On the other hand CUDA is only meant for (and can only work on)
Nvidia GPUs.

So while CUDA is older, and in the past has therefore been more mature / had a
more fully fledged eco-system, OpenCL is catching up, closing that gap of
difference.

CUDA compiles both host and device code at compile time, while OpenCL compiles
device code at run-time, to enable its potential of being multi-platform.

        \item Table with CUDA and OpenCL equivalents:
        \begin{table}[h]
        \begin{center}
            \begin{tabular}{|c|c|}
            \hline
                 \textbf{CUDA}          & \textbf{OpenCL}                                               \\ \hline
                 \textit{thread}        & $\Rightarrow$ \textit{·∫Åork-item}                              \\ \hline
                 \textit{thread block}  & $\Rightarrow$ \textit{work-group}                             \\ \hline
                 \textit{shared memory} & $\Rightarrow$ \textit{local memory $\Rightarrow$ \_local}     \\ \hline
                 \textit{local memory}  & $\Rightarrow$ \textit{private memory $\Rightarrow$ \_private} \\ \hline
            \end{tabular}
        \end{center}
        \end{table}

        \item There is no equivalent OpenCL term for the CUDA/Nvidia term
\textit{warp}, because OpenCL is meant to be multi-platform, while warp is
hardware defined in CUDA GPUs. AMD has something similar in its GPUs, called
\textit{wavefront}. These two properties are how many threads execute the same
instructions on the device, respectively 32 and 64 threads (though this can
change).

Since this property is hardware defined, OpenCL does not have an equivalent due
to its intentions of being multi-platform. There would be no ``multi-platform''
properties of a program that can only run on either CUDA or AMD platforms, due
to utilizing the warp/wavefront properties of either platform.

    \end{enumerate}

\section*{Problem 2, Heterogeneous computing}
    \begin{enumerate}[a)]

        \item $w$ is the total work done. $r$ can then be the work done on the
GPU, and $c$ the work done on the CPU, giving $c = w - r$, $0 < r < w$.

If, and only if, the only difference between the CPU and GPU is the time it
takes to run a computationally identical workset, then the division of labor
that results in the minimum execution time will be $1-\frac{1}{10+1}$ for the
GPU, and $1-\frac{10}{10+1}$ for the CPU.

        \begin{align*}
            T_{CPU}(w) &= T_{GPU}(w) \\
            10c &= 1r \\
            r &= 10(w-r) \\
            11r &= 10w \\
            r &= \frac{10}{11}w
        \end{align*}

        \item
        \begin{align*}
            T_{CPU}(w) &= T_{GPU}(w) + T_{in}(r) + T_{out}(w) \\
            10c &= 1r + 0.1r + 0.2r \\
            10c &= 1.3r \\
            r &= \frac{10}{11.3}w
        \end{align*}

        \item The GPU variable used in previous calculations is now represented
as $r_1$ and $r_2$, since the time it takes to execute on the CPU should be
equal to the time it takes to execute on each of the GPUs: $T_{CPU}(w) =
T_{GPU1}(w) = T_{GPU2}(w)$.

Since $T_{GPU2}(w)$ is twice as fast as $T_{GPU1}(w)$ $(T_{GPU1}(w) = w,
T_{GPU2}(w) = 2w, \Rightarrow 2w = 2*w)$, we can now express $r_1$, through the
$r_2$, and vica versa.

        \begin{align*}
            T_{CPU}(w) &= T_{GPU1}(w) \\
            10c &= r_1 \\
            r_1 &= 10(w - r_1 - r_2) \\
            r_1 &= 10(w - 3r_1) \\
            31r_1 &= 10w \\
            r_1 &= \frac{10}{31}w
        \end{align*}

        \begin{align*}
            T_{CPU}(w) &= T_{GPU2}(w) \\
            10c &= r_2 \\
            r_2 &= 10(w - 1.5r_2) \\
            16r_2 &= 10w \\
            r_2 &= \frac{10}{16}w
        \end{align*}

        \begin{align*}
            c &= w - \frac{10}{16}w - \frac{10}{31}w \\
            c &= w - \frac{31\times 10}{31\times 16} - \frac{16\times 10}
{16\times 31} \\
            c &= w - \frac{310}{496}w - \frac{160}{496}w \\
            c &= w(1-\frac{470}{496}) \\
            c &= \frac{26}{496}w = \frac{13}{248}w
        \end{align*}

        $c + r_1 + r_2 = \frac{13}{248}w + \frac{10}{31}w + \frac{10}{16}w = w$,
hence this division of labor is the optimimum for minimizing run-time when the
system has a CPU and two GPUs with these capabilities.

    \end{enumerate}

\vfill
\hfill \large{\today}
\end{document}
